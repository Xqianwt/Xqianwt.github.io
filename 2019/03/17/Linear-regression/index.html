<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>Linear_regression | 左萝卜右刀</title><meta name="description" content="Linear_regression"><meta name="keywords" content="机器学习,python"><meta name="author" content="Xqianwt"><meta name="copyright" content="Xqianwt"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://www.xqianwt.cn/2019/03/17/Linear-regression/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Linear_regression"><meta name="twitter:description" content="Linear_regression"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/Photo/avatar.png"><meta property="og:type" content="article"><meta property="og:title" content="Linear_regression"><meta property="og:url" content="http://www.xqianwt.cn/2019/03/17/Linear-regression/"><meta property="og:site_name" content="左萝卜右刀"><meta property="og:description" content="Linear_regression"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/Photo/avatar.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"cookieDomain":"https://jerryc.me/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: '添加书签',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-regression"><span class="toc-number">1.</span> <span class="toc-text">Linear_regression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Logistic-回归"><span class="toc-number">1.1.</span> <span class="toc-text">Logistic 回归</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#理论基础"><span class="toc-number">1.1.1.</span> <span class="toc-text">理论基础</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#代码实现"><span class="toc-number">1.1.2.</span> <span class="toc-text">代码实现</span></a></li></ol></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://p0.piqsels.com/preview/95/156/837/abstract-art-atom-background.jpg)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">左萝卜右刀</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad" data-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/Photo/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'"><div class="mobile_author-info__description"></div></div><hr><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">Linear_regression</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2019-03-17<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2019-07-17</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/机器学习/">机器学习</a></span></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h2 id="Linear-regression"><a href="#Linear-regression" class="headerlink" title="Linear_regression"></a>Linear_regression</h2><h3 id="Logistic-回归"><a href="#Logistic-回归" class="headerlink" title="Logistic 回归"></a>Logistic 回归</h3><h4 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h4><p>这是周志华的《机器学习》一书中<code>对数几率回归</code>的编程实现，主要使用<code>牛顿迭代法</code>。<br></p>
<p>对于二分类问题，输出形式为$y\in \lbrack0,1\rbrack$,而线性回归模型产生的预测值$z=w^Tx+b$是实值，利用<code>单位阶跃函数</code>将$z$转换为$0/1$值:<br>$$y=\left{\begin{aligned}<br> 0, &amp;&amp; \text{$z&lt;0$} \<br> 0.5, &amp;&amp; z=0 \<br> 1, &amp;&amp; z&gt;0<br>\end{aligned}<br>\right.$$<br>常用替代函数$y= \frac{1}{1+e^{-z}}$,即：<br>$$<br>ln\frac{y}{1-y}=w^Tx+b<br>$$<br>$w^Tx+b$简写为$\beta$，可利用<code>极大似然法</code>估计$w$和$b$:<br>$$<br>l(\beta)=\sum_{i=1}^n(-y_i\beta^T\hat x_i+ln(1+e^{\beta^T\hat{x_i}}))<br>$$<br>上式可用<code>梯度下降法</code>或者<code>牛顿法</code>求解。</p>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>这里主要用的牛顿法：<br>定义Sigmoid函数： </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Sigmoid</span><span class="params">(self, x)</span>:</span>  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br></pre></td></tr></table></figure>

<p>计算梯度算子与Hessian矩阵:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Hessian</span><span class="params">(self, beta, length)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算梯度算子与Hessian矩阵</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    input_data = self.input_data</span><br><span class="line">    label = self.label</span><br><span class="line">    gradient = np.mat([<span class="number">0</span>]*length, dtype = np.float64).T</span><br><span class="line">    hessian_mat = np.mat([[<span class="number">0</span>]*length]*length, dtype = np.float64)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(input_data)):</span><br><span class="line">        row = np.mat(list(input_data.iloc[i])).T</span><br><span class="line">        pro0 = float(self.Sigmoid(-beta*row))</span><br><span class="line">        gradient += row*(<span class="number">1</span>-pro0-label[i])</span><br><span class="line">        hessian_mat += row * row.T *(<span class="number">1</span>-pro0)*pro0</span><br><span class="line">    <span class="keyword">return</span> gradient, hessian_mat</span><br></pre></td></tr></table></figure>

<p>牛顿迭代法求解：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Newton</span><span class="params">(self, iter_num = <span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    牛顿迭代法求解</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    input_data = self.input_data</span><br><span class="line">    length =len(input_data.iloc[<span class="number">0</span>])</span><br><span class="line">    beta_init = np.mat([<span class="number">0</span>]*length)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iter_num):</span><br><span class="line">        gradient, hessian = self.Hessian(beta_init, length)</span><br><span class="line">        beta = beta_init - (hessian.I * gradient).T</span><br><span class="line">        beta_init = beta</span><br><span class="line">    <span class="keyword">return</span> beta</span><br></pre></td></tr></table></figure>

<p>预测方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Predict</span><span class="params">(self, test_data, iter_num = <span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    预测方法</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    beta = self.Newton(iter_num)</span><br><span class="line">    test_p = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(test_data)):       </span><br><span class="line">        p =  float(np.mat(test_data.iloc[i])*beta.T)</span><br><span class="line">        p =  self.Sigmoid(p)</span><br><span class="line">        <span class="keyword">if</span> p&gt;=<span class="number">0.5</span>:</span><br><span class="line">            p = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>: p = <span class="number">0</span></span><br><span class="line">        test_p.append(p</span><br><span class="line">    <span class="keyword">return</span> test_p</span><br></pre></td></tr></table></figure>

<p>结果演示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Train_Pre</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    训练集结果以及正确率</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    input_data = self.input_data</span><br><span class="line">    label = self.label</span><br><span class="line">    train_p = self.Predict(input_data)  </span><br><span class="line">    T = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> list(zip(train_p, label)):</span><br><span class="line">        <span class="keyword">if</span> i == j: T += <span class="number">1</span></span><br><span class="line">    true_rate = T/len(label)</span><br><span class="line">    <span class="keyword">return</span> train_p, true_rate</span><br></pre></td></tr></table></figure>

<p><img src="/.cn//Figure_1.png" alt><br></p>
<p>完整代码：<a href="https://github.com/Xqianwt/Linear_regression" target="_blank" rel="noopener">https://github.com/Xqianwt/Linear_regression</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Xqianwt</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://www.xqianwt.cn/2019/03/17/Linear-regression/">http://www.xqianwt.cn/2019/03/17/Linear-regression/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://www.xqianwt.cn">左萝卜右刀</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/机器学习/">机器学习    </a><a class="post-meta__tags" href="/tags/python/">python    </a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/Photo/avatar.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-buttom"><i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lozad post-qr-code__img" data-src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lozad post-qr-code__img" data-src="/img/alipay.jpg"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"></div><div class="clear_both"></div></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By Xqianwt</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="阅读模式"> </i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换">簡</a><i class="fa fa-moon-o nightshift" id="nightshift" title="夜间模式"></i></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script></body></html>